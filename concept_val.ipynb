{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6d4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch msprime matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce4961eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'predict_tmrca' from 'simple' (/Users/larry/Lab/DLCoal/simple.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msimple\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleCoalNN, ImprovedCoalNN, CoalescentDataset, predict_tmrca, plot_results\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'predict_tmrca' from 'simple' (/Users/larry/Lab/DLCoal/simple.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import msprime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from simple import SimpleCoalNN, ImprovedCoalNN, CoalescentDataset, predict_tmrca, plot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18fd0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(args):\n",
    "    \"\"\"用于并行处理的单个批次生成函数\"\"\"\n",
    "    batch_id, num_samples, sequence_length, Ne, mut_rate_range, rec_rate_range = args\n",
    "    np.random.seed(42 + batch_id)  # 确保不同进程有不同的随机种子\n",
    "    \n",
    "    all_haplotypes = []\n",
    "    all_tmrca = []\n",
    "    \n",
    "    # 创建不同的人口历史场景\n",
    "    demographic_models = [\n",
    "        # 常数大小人口\n",
    "        msprime.Demography(),\n",
    "        \n",
    "        # 人口扩张\n",
    "        msprime.Demography.isolated_model([1.0], \n",
    "            growth_rate=[0.01],\n",
    "            initial_size=[Ne]),\n",
    "            \n",
    "        # 人口瓶颈\n",
    "        msprime.Demography.population_split(time=0.1*Ne, \n",
    "            initial_size=[Ne*0.5, Ne], \n",
    "            growth_rate=[0, 0], \n",
    "            demographics_id=\"bottleneck\")\n",
    "    ]\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # 随机选择人口模型\n",
    "        demog_model = np.random.choice(demographic_models)\n",
    "        \n",
    "        # 随机调整变异率和重组率（增加多样性）\n",
    "        mut_rate = np.random.uniform(*mut_rate_range)\n",
    "        rec_rate = np.random.uniform(*rec_rate_range)\n",
    "        \n",
    "        # 随机有效群体大小\n",
    "        effective_size = Ne * np.random.uniform(0.8, 1.2)\n",
    "        \n",
    "        # 生成树序列\n",
    "        ts = msprime.simulate(\n",
    "            sample_size=2,\n",
    "            Ne=effective_size,\n",
    "            length=sequence_length,\n",
    "            mutation_rate=mut_rate,\n",
    "            recombination_rate=rec_rate,\n",
    "            record_full_arg=True,\n",
    "            demography=demog_model if demog_model.populations else None\n",
    "        )\n",
    "        \n",
    "        # 提取单倍型\n",
    "        haplotypes = np.zeros((2, sequence_length), dtype=np.int8)\n",
    "        for variant in ts.variants():\n",
    "            pos = int(variant.position)\n",
    "            if pos < sequence_length:\n",
    "                haplotypes[:, pos] = variant.genotypes\n",
    "        \n",
    "        # 提取TMRCA\n",
    "        tmrca = np.zeros(sequence_length)\n",
    "        for tree in ts.trees():\n",
    "            left, right = int(tree.interval.left), int(min(tree.interval.right, sequence_length))\n",
    "            tmrca[left:right] = tree.tmrca(0, 1) * 2 * effective_size\n",
    "        \n",
    "        all_haplotypes.append(haplotypes)\n",
    "        all_tmrca.append(tmrca)\n",
    "    \n",
    "    return np.array(all_haplotypes), np.array(all_tmrca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f80644f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_generate_data(total_samples, sequence_length, Ne=10000, \n",
    "                          mut_rate_range=(0.5e-8, 5e-8), \n",
    "                          rec_rate_range=(0.5e-8, 5e-8),\n",
    "                          batch_size=1000, num_processes=None):\n",
    "    \"\"\"\n",
    "    并行生成大量模拟数据\n",
    "    \n",
    "    参数:\n",
    "    total_samples: 总样本数\n",
    "    sequence_length: 序列长度\n",
    "    Ne: 有效群体大小\n",
    "    mut_rate_range: 突变率范围 (min, max)\n",
    "    rec_rate_range: 重组率范围 (min, max)\n",
    "    batch_size: 每个批次生成的样本数\n",
    "    num_processes: 并行进程数，默认为可用CPU核心数\n",
    "    \n",
    "    返回:\n",
    "    haplotypes: 单倍型数据\n",
    "    tmrca: 对应的TMRCA值\n",
    "    \"\"\"\n",
    "    if num_processes is None:\n",
    "        num_processes = mp.cpu_count()\n",
    "    \n",
    "    print(f\"使用{num_processes}个进程并行生成数据\")\n",
    "    \n",
    "    # 计算需要多少批次\n",
    "    num_batches = (total_samples + batch_size - 1) // batch_size\n",
    "    last_batch_size = total_samples - (num_batches - 1) * batch_size\n",
    "    \n",
    "    # 准备参数列表\n",
    "    args_list = []\n",
    "    for i in range(num_batches - 1):\n",
    "        args_list.append((i, batch_size, sequence_length, Ne, mut_rate_range, rec_rate_range))\n",
    "    # 添加最后一个可能不完整的批次\n",
    "    if last_batch_size > 0:\n",
    "        args_list.append((num_batches - 1, last_batch_size, sequence_length, Ne, mut_rate_range, rec_rate_range))\n",
    "    \n",
    "    # 并行生成数据\n",
    "    all_haplotypes = []\n",
    "    all_tmrca = []\n",
    "    \n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        results = list(tqdm(pool.imap(generate_batch, args_list), \n",
    "                           total=len(args_list), \n",
    "                           desc=\"生成数据批次\"))\n",
    "    \n",
    "    # 收集结果\n",
    "    for haps, tmrcas in results:\n",
    "        all_haplotypes.append(haps)\n",
    "        all_tmrca.append(tmrcas)\n",
    "    \n",
    "    return np.concatenate(all_haplotypes), np.concatenate(all_tmrca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2251182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_in_chunks(haplotypes, tmrca, output_prefix, chunk_size=10000):\n",
    "    \"\"\"将大型数据集分块保存到磁盘\"\"\"\n",
    "    num_samples = haplotypes.shape[0]\n",
    "    num_chunks = (num_samples + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, num_samples)\n",
    "        \n",
    "        chunk_data = {\n",
    "            'haplotypes': haplotypes[start_idx:end_idx],\n",
    "            'tmrca': tmrca[start_idx:end_idx]\n",
    "        }\n",
    "        \n",
    "        chunk_file = f\"{output_prefix}_chunk_{i}.npz\"\n",
    "        np.savez_compressed(chunk_file, **chunk_data)\n",
    "        print(f\"保存数据块 {i+1}/{num_chunks} 到 {chunk_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c082568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_chunks(input_prefix, num_chunks):\n",
    "    \"\"\"从多个数据块文件加载数据\"\"\"\n",
    "    all_haplotypes = []\n",
    "    all_tmrca = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        chunk_file = f\"{input_prefix}_chunk_{i}.npz\"\n",
    "        if os.path.exists(chunk_file):\n",
    "            data = np.load(chunk_file)\n",
    "            all_haplotypes.append(data['haplotypes'])\n",
    "            all_tmrca.append(data['tmrca'])\n",
    "            print(f\"加载数据块 {i+1}/{num_chunks} 从 {chunk_file}\")\n",
    "        else:\n",
    "            print(f\"警告: 数据块 {chunk_file} 未找到\")\n",
    "    \n",
    "    return np.concatenate(all_haplotypes), np.concatenate(all_tmrca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6e178ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(model, train_loader, val_loader, max_epochs, \n",
    "                             patience=5, learning_rate=0.001, \n",
    "                             device='cuda', checkpoint_dir='checkpoints'):\n",
    "    \"\"\"\n",
    "    训练模型，带有提前停止和检查点保存功能\n",
    "    \n",
    "    参数:\n",
    "    model: 要训练的模型\n",
    "    train_loader: 训练数据加载器\n",
    "    val_loader: 验证数据加载器\n",
    "    max_epochs: 最大训练轮数\n",
    "    patience: 提前停止的耐心值\n",
    "    learning_rate: 学习率\n",
    "    device: 训练设备\n",
    "    checkpoint_dir: 检查点保存目录\n",
    "    \n",
    "    返回:\n",
    "    训练好的模型、训练历史和最佳模型状态\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "        \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{max_epochs} [Train]\")):\n",
    "            inputs = batch['input'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # 定期打印进度\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"  Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{max_epochs} [Val]\"):\n",
    "                inputs = batch['input'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 记录历史\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1}/{max_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Time: {epoch_time:.2f}s')\n",
    "        \n",
    "        # 保存中间检查点\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'focus_input_size': model.focus_input_size if hasattr(model, 'focus_input_size') else None,\n",
    "                'input_features': 7,  # 我们现在使用7个特征\n",
    "                'root_time': model.root_time if hasattr(model, 'root_time') else None,\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt'))\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = {\n",
    "                'model': model.state_dict(),\n",
    "                'focus_input_size': model.focus_input_size if hasattr(model, 'focus_input_size') else None,\n",
    "                'input_features': 7,  # 我们现在使用7个特征\n",
    "                'root_time': model.root_time if hasattr(model, 'root_time') else None,\n",
    "                'epoch': epoch\n",
    "            }\n",
    "            # 保存最佳模型\n",
    "            torch.save(best_model_state, os.path.join(checkpoint_dir, 'best_model.pt'))\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"提前停止! {patience}轮验证损失没有改善\")\n",
    "                break\n",
    "    \n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(best_model_state['model'])\n",
    "    \n",
    "    return model, history, best_model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2e9999c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n",
      "生成新的训练数据...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 41\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m生成新的训练数据...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m     train_haplotypes, train_tmrca \u001b[38;5;241m=\u001b[39m parallel_generate_data(\n\u001b[1;32m     36\u001b[0m         train_samples, \n\u001b[1;32m     37\u001b[0m         sequence_length,\n\u001b[1;32m     38\u001b[0m         mut_rate_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.5e-8\u001b[39m, \u001b[38;5;241m5e-8\u001b[39m),  \u001b[38;5;66;03m# 更大的突变率范围\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         rec_rate_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.5e-8\u001b[39m, \u001b[38;5;241m5e-8\u001b[39m),\n\u001b[1;32m     40\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m,\n\u001b[0;32m---> 41\u001b[0m         num_processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m16\u001b[39m, mp\u001b[38;5;241m.\u001b[39mcpu_count())  \u001b[38;5;66;03m# 限制使用的进程数\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m     save_data_in_chunks(train_haplotypes, train_tmrca, train_prefix, chunk_size)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_chunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(val_chunks)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mp' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 设置随机种子以便结果可重复\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # 检测CUDA是否可用\n",
    "    device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'使用设备: {device}')\n",
    "    \n",
    "    # 极大数据生成参数 \n",
    "    train_samples = 200000  # 20万个样本用于训练\n",
    "    val_samples = 20000    # 2万个样本用于验证\n",
    "    test_samples = 1000   # 1万个样本用于测试\n",
    "    sequence_length = 20000  # 每个序列2000个位点\n",
    "    chunk_size = 10000     # 每个数据块大小\n",
    "    \n",
    "    # 检查是否已经有保存的数据\n",
    "    train_chunks = (train_samples + chunk_size - 1) // chunk_size\n",
    "    val_chunks = (val_samples + chunk_size - 1) // chunk_size\n",
    "    test_chunks = (test_samples + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    data_dir = 'large_data'\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    train_prefix = os.path.join(data_dir, 'train')\n",
    "    val_prefix = os.path.join(data_dir, 'val')\n",
    "    test_prefix = os.path.join(data_dir, 'test')\n",
    "    \n",
    "    if all(os.path.exists(f\"{train_prefix}_chunk_{i}.npz\") for i in range(train_chunks)):\n",
    "        print(\"加载已存在的训练数据...\")\n",
    "        train_haplotypes, train_tmrca = load_data_from_chunks(train_prefix, train_chunks)\n",
    "    else:\n",
    "        print(\"生成新的训练数据...\")\n",
    "        train_haplotypes, train_tmrca = parallel_generate_data(\n",
    "            train_samples, \n",
    "            sequence_length,\n",
    "            mut_rate_range=(0.5e-8, 5e-8),  # 更大的突变率范围\n",
    "            rec_rate_range=(0.5e-8, 5e-8),\n",
    "            batch_size=5000,\n",
    "            num_processes=min(16, mp.cpu_count())  # 限制使用的进程数\n",
    "        )\n",
    "        save_data_in_chunks(train_haplotypes, train_tmrca, train_prefix, chunk_size)\n",
    "    \n",
    "    if all(os.path.exists(f\"{val_prefix}_chunk_{i}.npz\") for i in range(val_chunks)):\n",
    "        print(\"加载已存在的验证数据...\")\n",
    "        val_haplotypes, val_tmrca = load_data_from_chunks(val_prefix, val_chunks)\n",
    "    else:\n",
    "        print(\"生成新的验证数据...\")\n",
    "        val_haplotypes, val_tmrca = parallel_generate_data(\n",
    "            val_samples, \n",
    "            sequence_length,\n",
    "            mut_rate_range=(0.5e-8, 5e-8),\n",
    "            rec_rate_range=(0.5e-8, 5e-8),\n",
    "            batch_size=5000,\n",
    "            num_processes=min(16, mp.cpu_count())\n",
    "        )\n",
    "        save_data_in_chunks(val_haplotypes, val_tmrca, val_prefix, chunk_size)\n",
    "    \n",
    "    if all(os.path.exists(f\"{test_prefix}_chunk_{i}.npz\") for i in range(test_chunks)):\n",
    "        print(\"加载已存在的测试数据...\")\n",
    "        test_haplotypes, test_tmrca = load_data_from_chunks(test_prefix, test_chunks)\n",
    "    else:\n",
    "        print(\"生成新的测试数据...\")\n",
    "        test_haplotypes, test_tmrca = parallel_generate_data(\n",
    "            test_samples, \n",
    "            sequence_length,\n",
    "            mut_rate_range=(0.5e-8, 5e-8),\n",
    "            rec_rate_range=(0.5e-8, 5e-8),\n",
    "            batch_size=5000,\n",
    "            num_processes=min(16, mp.cpu_count())\n",
    "        )\n",
    "        save_data_in_chunks(test_haplotypes, test_tmrca, test_prefix, chunk_size)\n",
    "    \n",
    "    print(f'训练样本: {train_haplotypes.shape[0]}')\n",
    "    print(f'验证样本: {val_haplotypes.shape[0]}')\n",
    "    print(f'测试样本: {test_haplotypes.shape[0]}')\n",
    "    print(f'序列长度: {sequence_length}')\n",
    "    \n",
    "    # 创建数据集\n",
    "    window_size = 30  # 增大局部特征的窗口大小\n",
    "    train_dataset = CoalescentDataset(train_haplotypes, train_tmrca, window_size=window_size)\n",
    "    val_dataset = CoalescentDataset(val_haplotypes, val_tmrca, window_size=window_size)\n",
    "    test_dataset = CoalescentDataset(test_haplotypes, test_tmrca, window_size=window_size)\n",
    "    \n",
    "    # 创建数据加载器，使用较大的batch size和更多的worker\n",
    "    batch_size = 256\n",
    "    num_workers = min(8, os.cpu_count() or 1)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                             num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, \n",
    "                           num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                            num_workers=num_workers)\n",
    "    \n",
    "    # 检查点存储目录\n",
    "    checkpoint_dir = 'large_checkpoints'\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    # 检查是否有保存的模型\n",
    "    model_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "    if os.path.exists(model_path) and os.path.getsize(model_path) > 0:\n",
    "        print(f'加载已训练模型 {model_path}')\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model = ImprovedCoalNN(focus_input_size=sequence_length,\n",
    "                               input_features=7,  # 使用7个特征\n",
    "                               hidden_dims=[64, 128, 256, 512, 1024],  # 更大的网络\n",
    "                               kernel_sizes=[9, 7, 5, 3, 3],  # 更大的感受野\n",
    "                               num_residual_blocks=3)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        # 创建更大的模型\n",
    "        model = ImprovedCoalNN(focus_input_size=sequence_length,\n",
    "                               input_features=7,  # 使用7个特征\n",
    "                               hidden_dims=[64, 128, 256, 512, 1024],  # 更大的网络\n",
    "                               kernel_sizes=[9, 7, 5, 3, 3],  # 更大的感受野\n",
    "                               num_residual_blocks=3)\n",
    "        \n",
    "        # 训练模型\n",
    "        print('开始训练大规模CoalNN模型...')\n",
    "        model, history, best_model = train_with_early_stopping(\n",
    "            model, \n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            max_epochs=100,  # 增加训练轮数\n",
    "            patience=10,     # 提前停止的耐心值\n",
    "            learning_rate=0.0005,  # 降低学习率以更好地收敛\n",
    "            device=device,\n",
    "            checkpoint_dir=checkpoint_dir\n",
    "        )\n",
    "        \n",
    "        # 绘制损失曲线\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(history['train_loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss (Large Scale)')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(checkpoint_dir, 'large_scale_loss.png'), dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "    # 在测试集上评估模型\n",
    "    print('在大规模测试集上评估模型...')\n",
    "    from torch.utils.data import Subset\n",
    "    \n",
    "    # 分批评估以避免内存问题\n",
    "    batch_size = 1000\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for i in range(0, len(test_dataset), batch_size):\n",
    "        end_idx = min(i + batch_size, len(test_dataset))\n",
    "        subset = Subset(test_dataset, range(i, end_idx))\n",
    "        subset_loader = DataLoader(subset, batch_size=128, num_workers=num_workers)\n",
    "        \n",
    "        print(f\"评估子集 {i}-{end_idx}...\")\n",
    "        pred, targ = evaluate_model(model, subset_loader, device=device)\n",
    "        all_predictions.append(pred)\n",
    "        all_targets.append(targ)\n",
    "    \n",
    "    # 合并结果\n",
    "    predictions = np.concatenate(all_predictions)\n",
    "    targets = np.concatenate(all_targets)\n",
    "    \n",
    "    # 可视化结果\n",
    "    print('可视化大规模结果...')\n",
    "    mse, mae, r2 = plot_results(predictions, targets, log_transform=True)\n",
    "    \n",
    "    print(f'大规模优化版CoalNN评估完成!')\n",
    "    print(f'MSE: {mse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
